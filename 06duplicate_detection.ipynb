{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eHtajNyBtiV"
   },
   "source": [
    "# IRRS Lab Session 6: Duplicate detection using simhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L1HwcMeBtiW"
   },
   "source": [
    "In this session you will:\n",
    "\n",
    "- implement the simhashing algorithm\n",
    "- apply it to the `arxiv` abstracts to detect near duplicates among documents in this index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLX0kIryBtiX"
   },
   "source": [
    "## 1. Simhash-based near-duplicate detection algorithm\n",
    "\n",
    "We depart from an indexed corpus, namely the one that we have used in previous sessions that contains 58102 abstracts from the\n",
    "scientific [arXiv.org](https://arxiv.org) repository. The following pseudocode uses $k,m$ as the usual parameters for lsh (locality sensitive hashing)\n",
    "scheme for gap amplification (repetition of $m$ lsh tables, using $k$ projections each).\n",
    "\n",
    "  1. For each word $t$ in the vocabulary $V$ we generate $b = k\\times m$ random $\\pm 1$ signs:\n",
    "      - generate a unique hash (bitstring) $h_t$ of size $b$\n",
    "      - let $h'_t$ be the hash above but replacing each 0 with -1.\n",
    "  \n",
    "  2. We view each document as a tf-idf weighted sum of its terms. Thus, to hash a document, we sum the weighted contributions of all its terms. Specifically, for each document in our corpus represented by list of non-zero entries $t, w_t$:\n",
    "      - compute $simhash(d)$ as sum $\\sum_t h'_t * w_t$. \n",
    "      - convert $simhash(d)$ to bit vector using the sign of entries (positive entries become 1, negative ones become 0)\n",
    "  \n",
    "  3. Slice simhashes into $m$ chunks of $k$ bits\n",
    "     - for each $k$-bit chunk:\n",
    "        - place the document into the bucket given by the chunk in the appropriate table. Note that we may identify each chunk with an integer index between 0 and 2^k - 1.\n",
    "  \n",
    "  4. Similar documents will likely fall on the same side of many hyperplanes, and thus share one or more $k$-bit chunks. Thus, the potential duplicates are pairs of documents that fall into the same bucket in **at least one** hash table. Now we need to\n",
    "     - compute cosine similarity among these potential duplicates, and\n",
    "     - show pairs in decreasing cosine similarity order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGGL-4DYBtiY"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:**  \n",
    "\n",
    "As a little exercise, suppose that a document $d$ contains _two_ non-zero entries, which correspond to terms `bit` with tf-idf weight $0.4$ and `coin` with tf-idf weight $1.2$.\n",
    "The md5 hash code for `bit` is 1111 and the md5 code for `coin` is 1001 (so $b=4$). Compute the binary simhash for this document with $k=4, m = 1$. (_Hint: result should be 1001_)\n",
    "\n",
    "**Exercise 1 (Solution):**\n",
    "\n",
    "$d=[0.4,1.2] \\quad ; \\quad h'_t(bit)=[1,1,1,1] \\quad ; \\quad h'_t(coin)=[1,-1,-1,1]$\n",
    "\n",
    "$simhash(d)=\\sum_t h'_t*w_t=[0.4,0.4,0.4,0.4]+[1.2,-1.2,-1.2,1.2]=[1.6,-0.8,-0.8,1.6]$\n",
    "\n",
    "$simhash_{bit}(d)=[1,0,0,1]$ (replace positive entries of $simhash(d)$ with 1 and negative entries with 0)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDcbkfpdBtiY"
   },
   "source": [
    "## 2. Hashing codes for terms in vocabulary\n",
    "\n",
    "To make your life easier, we provide code for generating hash codes for the terms in the vocabulary.\n",
    "The given function takes as input the size of the hash, as well as the term itself.\n",
    "\n",
    "We use a fast deterministic hash (MD5) to get repeatable pseudo-random bits. Even though it is actually deterministic, we assume that in practice its output behaviour is sufficiently similar to that of a truly random hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVL3EjNsBtiZ",
    "outputId": "443f054d-1954-4969-8d38-3d5ffa4e84e6"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def _termhash(x : str, b : int) -> str:\n",
    "    \"\"\"returns bitstring of size b based on md5 algorithm\"\"\"\n",
    "    assert b <= 128, 'this encoding scheme supports hashes of length at most 128; try smaller b'\n",
    "    h = hashlib.md5(x.encode('utf8')).digest()\n",
    "    return ''.join(format(byte, '08b') for byte in h)[:b]\n",
    "\n",
    "for w in ['bit', 'coin', 'hola que tal']:\n",
    "    print(f'hash.md5({w}) = {_termhash(w, 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Js1nfaZyBtia"
   },
   "source": [
    "## 3. Loading tf-idf for the whole corpus\n",
    "\n",
    "We provide the document-term matrix with normalized tf-idf weights with a sparse representation.\n",
    "This is so that further code does not need to check for tf-idf weights against elasticsearch which is very slow.\n",
    "\n",
    "Please download the corpus data from https://www.cs.upc.edu/~caim/05corpus.pkl and run the following cells to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBnSpUclBtib",
    "outputId": "2fda5552-660b-4e96-aac2-fb729ee06880"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname = '05corpus.pkl'\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# show contents\n",
    "\n",
    "print(f\"Loaded corpus with {len(corpus)} documents.\")\n",
    "all_document_ids = list(corpus.keys())\n",
    "print(f\"A few document ids are: {sorted(random.sample(all_document_ids, 10))}\")\n",
    "print(f\"Each document is a dictionary with term -> tf-idf weights, e.g. the first:\")\n",
    "print(corpus[all_document_ids[0]])\n",
    "\n",
    "# obtain histogram with nr of terms in each document\n",
    "values = [len(corpus[id]) for id in all_document_ids]\n",
    "mu = np.mean(values)\n",
    "sigma = np.std(values)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.histplot(values, bins=50, edgecolor='black', stat='density', label='Distribution of values')\n",
    "plt.title(f'Histogram of number of types per document with Gaussian Overlay\\nMean = ${mu:.2f}$, Std Dev = ${sigma:.2f}$', fontsize=12)\n",
    "xmin, xmax = plt.xlim()\n",
    "x_axis = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "# Calculate the probability density function (PDF) for the x-values.\n",
    "plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), 'red', linewidth=2, label='Gaussian Overlay')\n",
    "plt.legend(); # Display the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laad5LXLBtib"
   },
   "source": [
    "## 4. Simhashing scheme and near-duplicate collection and verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byaKdQmdBtib"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Write the `simhash` function provided below. Make sure you understand that each bit of the simhash encodes which \"side\" the tf-idf document representation falls on a (pseudo-)random projection. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A2pBq50Btic"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "## constants\n",
    "K = 18\n",
    "M = 3\n",
    "B = M*K\n",
    "\n",
    "def _simhash(id, b):\n",
    "    \"\"\"\n",
    "        id is the document id, b is the desired length of the simhash\n",
    "        it should return a bitstring of length b as explained in the\n",
    "        first section of the notebook\n",
    "    \"\"\"\n",
    "    simhash = np.zeros(b)\n",
    "    for term in list(corpus[id].keys()):\n",
    "        # print(term)\n",
    "        ht = list(_termhash(term, b))\n",
    "        # print(ht)\n",
    "        ht_prime = np.array([1 if bit == \"1\" else -1 for bit in ht], dtype=float)\n",
    "        # print(ht_prime)\n",
    "\n",
    "        # print(ht_prime, corpus[id][term])\n",
    "        simhash += ht_prime * corpus[id][term]\n",
    "        # print(f\"simhash: {simhash}\")\n",
    "\n",
    "    simhash_bit = [1 if entry >= 0 else 0 for entry in simhash]\n",
    "    # print(f\"simhash bit: {simhash_bit}\")\n",
    "\n",
    "    return \"\".join([str(sb) for sb in simhash_bit])\n",
    "\n",
    "# simhash stores all simhashes of the whole corpus in a dictionary\n",
    "simhash = {id: _simhash(id, B) for id in tqdm(corpus)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G83Kpd8yBtic"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 3:**  \n",
    "\n",
    "Based on simhashes, write code that places the documents in their corresponding lsh hash tables' buckets across $m$ repetitions. Once, the lsh hash tables have been\n",
    "populated, find all potential near-duplicate candidate pairs. Check among all candidates their real cosine similarity and based on this determine true positives\n",
    "(pairs of documents that are indeed similar and have collided in some table), and false positives (pairs of documents that are not similar but have collided in some table).\n",
    "Compute speed of the method vs. false positives / true positives for different values of $m$ and $k$. Make sure that $k$ is not tiny (say greater than 10) if you want\n",
    "the method to be fast.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def build_lsh_tables(simhash, m, k):\n",
    "    tables = [dict() for _ in range(m)]\n",
    "    for doc_id, hash_bits in simhash.items():\n",
    "        for i in range(m):\n",
    "            chunk = hash_bits[i*k : (i+1)*k]\n",
    "            bucket_id = int(''.join(map(str, chunk)), 2)  # 0 .. 2^k - 1\n",
    "            tables[i].setdefault(bucket_id, []).append(doc_id)\n",
    "    return tables\n",
    "\n",
    "tables = build_lsh_tables(simhash, M, K)\n",
    "# pprint(tables[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from pprint import pprint\n",
    "\n",
    "def collect_candidate_pairs(tables):\n",
    "    candidates = set() # set notion to ensure no duplicates\n",
    "    for table in tables:\n",
    "        for bucket_id, doc_ids in table.items():\n",
    "            if len(doc_ids) < 2:\n",
    "                continue\n",
    "            for a, b in combinations(sorted(doc_ids), 2):\n",
    "                candidates.add((a, b))\n",
    "    return candidates\n",
    "\n",
    "candidate_pairs = collect_candidate_pairs(tables)\n",
    "print(f\"Candidate pairs count: {len(candidate_pairs)}\")\n",
    "pprint(list(candidate_pairs)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "def cosine_similarity(doc_vec_a, doc_vec_b):\n",
    "    if not doc_vec_a or not doc_vec_b:\n",
    "        return 0.0\n",
    "\n",
    "    common_terms = set(doc_vec_a.keys()) & set(doc_vec_b.keys())\n",
    "    dot = sum(doc_vec_a[t] * doc_vec_b[t] for t in common_terms)\n",
    "\n",
    "    norm_a = math.sqrt(sum(w*w for w in doc_vec_a.values()))\n",
    "    norm_b = math.sqrt(sum(w*w for w in doc_vec_b.values()))\n",
    "    if norm_a == 0.0 or norm_b == 0.0:\n",
    "        return 0.0\n",
    "    return dot / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pairs(candidate_pairs, corpus, min_sim):\n",
    "    true_positives = []\n",
    "    false_positives = []\n",
    "    for a, b in candidate_pairs:\n",
    "        sim = cosine_similarity(corpus[a], corpus[b])\n",
    "        if sim >= min_sim:\n",
    "            true_positives.append((a, b, sim))\n",
    "        else:\n",
    "            false_positives.append((a, b, sim))\n",
    "    # Sort (readability)\n",
    "    true_positives.sort(key=lambda x: x[2], reverse=True)\n",
    "    false_positives.sort(key=lambda x: x[2], reverse=True)\n",
    "    return true_positives, false_positives\n",
    "\n",
    "min_sim = 0.7\n",
    "tp, fp = classify_pairs(candidate_pairs, corpus, min_sim=min_sim)\n",
    "print(f\"True positives (>= {min_sim}): {len(tp)} | False positives (< {min_sim}): {len(fp)}\")\n",
    "print(\"Top 5 TP:\")\n",
    "pprint(tp[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark speed vs TP/FP for different (M, K) configurations\n",
    "lsh_configs = [\n",
    "    (3, 12),\n",
    "    (3, 18),\n",
    "    (4, 16),\n",
    "    (5, 12),\n",
    "]\n",
    "\n",
    "benchmark_results = []\n",
    "for num_tables, bits_per_chunk in lsh_configs:\n",
    "    total_bits = num_tables * bits_per_chunk\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    # Recompute simhash for these parameters using the previously defined _simhash\n",
    "    simhash_for_config = {doc_id: _simhash(doc_id, total_bits) for doc_id in corpus}\n",
    "\n",
    "    lsh_tables = build_lsh_tables(simhash_for_config, num_tables, bits_per_chunk)\n",
    "    candidate_pairs_for_config = collect_candidate_pairs(lsh_tables)\n",
    "    true_positives_for_config, false_positives_for_config = classify_pairs(candidate_pairs_for_config, corpus, min_sim=0.8)\n",
    "    elapsed_seconds = time.perf_counter() - t_start\n",
    "\n",
    "    br = {\n",
    "        \"M\": num_tables,\n",
    "        \"K\": bits_per_chunk,\n",
    "        \"time_sec\": round(elapsed_seconds, 3),\n",
    "        \"candidates\": len(candidate_pairs_for_config),\n",
    "        \"tp\": len(true_positives_for_config),\n",
    "        \"fp\": len(false_positives_for_config),\n",
    "        \"precision\": round(len(true_positives_for_config) / max(1, (len(true_positives_for_config) + len(false_positives_for_config))), 3),\n",
    "    }\n",
    "\n",
    "    pprint(br)\n",
    "\n",
    "    benchmark_results.append(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_benchmarks(benchmark_results):\n",
    "    if not benchmark_results:\n",
    "        print(\"No benchmark results to visualize.\")\n",
    "        return\n",
    "\n",
    "    labels = [f\"M={br['M']}, K={br['K']}\" for br in benchmark_results]\n",
    "    times = [br['time_sec'] for br in benchmark_results]\n",
    "    candidates = [br['candidates'] for br in benchmark_results]\n",
    "    precision = [br['precision'] for br in benchmark_results]\n",
    "    tp = [br['tp'] for br in benchmark_results]\n",
    "    fp = [br['fp'] for br in benchmark_results]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('LSH Simhash Benchmark Summary', fontsize=14)\n",
    "\n",
    "    # Precision\n",
    "    ax = axes[0][0]\n",
    "    ax.bar(labels, precision, color=\"#4C9F70\")\n",
    "    ax.set_ylabel('Precision (TP / (TP+FP))')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Precision by (M, K)')\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "\n",
    "    # Time\n",
    "    ax = axes[0][1]\n",
    "    ax.bar(labels, times, color=\"#3D5A80\")\n",
    "    ax.set_ylabel('Time (sec)')\n",
    "    ax.set_title('Runtime by (M, K)')\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "\n",
    "    # Candidates\n",
    "    ax = axes[1][0]\n",
    "    ax.bar(labels, candidates, color=\"#EE6C4D\")\n",
    "    ax.set_ylabel('# Candidate Pairs')\n",
    "    ax.set_title('Candidates by (M, K)')\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "\n",
    "    # TP vs FP stacked bar\n",
    "    ax = axes[1][1]\n",
    "    ax.bar(labels, tp, label='TP', color=\"#2A9D8F\")\n",
    "    ax.bar(labels, fp, bottom=tp, label='FP', color=\"#E76F51\")\n",
    "    ax.set_ylabel('Count (TP + FP)')\n",
    "    ax.set_title('True vs False Positives')\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "visualize_benchmarks(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjVspJeQBtic"
   },
   "source": [
    "## 5. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like the important parts of your implementation (data structures used for representing objects, algorithms used, etc). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title. Include your code in your submission.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html); see date at the raco's submissions page."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
